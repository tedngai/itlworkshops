---
layout: post
title: Natural Language Processing Part 1 Data Cleanup & Exploratory Data Analysis
description: Python, Pandas, BeautifulSoup, Regular Expression
image: 
---
![AlexJones](../../assets/images/NLP/AlexJones.jpg)
## Project Description

This is part 1 of the Natural Language Processing module. The process described here largely follows the structure of Alice Zhao's (A Dash of Data) [Presentation on NLP at PyOhio 2018](https://www.youtube.com/watch?v=xvqsFTUsOmc){:target="_blank"}. This first part will mainly focused on webscraping and cleaning up data for Natural Language Processing. We will also deviate from Alice's presentation and scrape our own dataset from [Joe Rogan's podcast transcript](https://jrescribe.com/){:target="blank"}. The transcript from Rogan's Podcast is preferred because of the length and the varieties of his interviews, we get some very interesting results. 

The main goal for this module is to introduce the idea of vectorization of data, an essential step in regression and classification, which is the precursor to machine learning. When face with data types such as word or sound, we need to find ways to represent the data with numbers, and the numerical representation would be called **Vectors** for 1-dimensional values, **Matrices** for 2-dimensional values, and **Tensors** for higher dimensions. NLP relies on a package in [SCIKIT-Learn](<https://scikit-learn.org/stable/>){:target="_blank"}, a very popular machine learning library in python, to convert words to vectors, which then enable us to do higher level analysis.  

Before you begin, you must have completed the installation and visualiation modules so you have working knowledge of anaconda and regular expression. 

***

# Step 1
## Download Data

For this exercise we will be using **BeautifulSoup** to scrape data off  [Joe Rogan's podcast transcript](https://jrescribe.com/){:target="blank"} site, and I have chosen 16 interviews based on the range of ideas being discussed. To get that started, import the following packages.

```python
import requests, pickle
from bs4 import BeautifulSoup
```

Next we will define a function to launch BeautifulSoup and scrape the data, a list of urls to the interview transcripts, a list of speakers, and a list of the full name of the speakers.

```python
def url_to_transcript(url):
    text=[]
    page = requests.get(url).text
    soup = BeautifulSoup(page, 'lxml')
    text = [p.text for p in soup.find(class_="content").find_all('p')]
    print(url)
    return text

urls = ['https://jrescribe.com/transcripts/p1255.html',
        'https://jrescribe.com/transcripts/p1245.html',
        'https://jrescribe.com/transcripts/p1233.html',
        'https://jrescribe.com/transcripts/p1213.html',
        'https://jrescribe.com/transcripts/p1227.html',
        'https://jrescribe.com/transcripts/p1216.html',
        'https://jrescribe.com/transcripts/p1236.html',
        'https://jrescribe.com/transcripts/p1234.html',
        'https://jrescribe.com/transcripts/p1198.html',
        'https://jrescribe.com/transcripts/p1184.html',
        'https://jrescribe.com/transcripts/p1169.html',
        'https://jrescribe.com/transcripts/p1121.html',
        'https://jrescribe.com/transcripts/p1260.html',
        'https://jrescribe.com/transcripts/p1248.html',
        'https://jrescribe.com/transcripts/p1241.html',
        'https://jrescribe.com/transcripts/p1240.html',
          ]

speakers = ['jones', 'yang','cox','weil','tyson','penrose','dorsey','sinclair',
            'brown','barr','musk','pollen','lewis','ottman','harris','galante',
           ]

full_names = ['Alex Jones', 'Andrew Yang', 'Brian Cox', 'Andrew Weil', 'Mike Tyson', 'Roger Penrose',
              'Jack Dorsey', 'David Sinclair','Darren Brown','Rosanne Barr','Elon Musk','Michael Pollen',
              'Lennox Lewis', 'Bill Ottman','Sam Harris','Forrest Galante']
```

`text = [p.text for p in soup.find(class_="content").find_all('p')]` This line in the function is the key that pulls out the text the whole interview. It finds the div tag **class="content"** which contains the transcript, and strips out all the html code. Next we call the function to store all the text into the variable **transcript**. This is a very Pythonic syntax which is a short form for writing a loop. In the long form, it is equivalent to:

```python
text=[]
for p in soup.find(class_="content").find_all('p'):
  text.append(p.text)
```

And we can even break this down further into simpler components, which is to say, first find the div class named content, then find all the paragraph tags in there, then put all those paragraphs into the text variable. 

```python
text=[]
a = soup.find(class_="content")
b = a.find_all('p')
for p in b:
  text.append(p.text)
```

Now let's get back on track, call the function to process all the urls and get all the text into the transcript variable.

```python
transcript = [url_to_transcript(u) for u in urls]
```

Again, notice the short form. This could have been written as-

```python
transcript = []
for u in urls:
  transcript.append(url_to_transcript(u))
```

Once its done scraping, now we **"pickle"** all the transcripts into our local storage. **Pickling** here means saving what ever is in the variable into a file, which can then be called later. The following code shows how to **pickle** the transcripts and using the **speakers** list to save each transcrption with their names. And later we retrieve the **pickled** data back and create a **dictionary** to correlate the **speakers** to the **transcripts**.

```python
for i, s in enumerate(speakers):
    with open('transcripts/' + s + ".txt", 'wb') as file:
        pickle.dump(transcript[i], file)
        
data = {}
for i, s in enumerate(speakers):
    with open('transcripts/' + s + '.txt', 'rb') as file:
        data[s] = pickle.load(file)
```

Typing the following and see that you now have a dictionary with **key:speakers** and **value:transcript** structure.

```python
data.keys()
```
However, you can see that the **transcripts** is a **list** of texts due to the function call **find_all('p')**, all html paragraphs with a **<p>** tag were imported as a separate list item. **[:2]** shows the beginning to the 3rd item in the list, and **[1:5]** will show the second to 6th item in the list. 
```python
data['weil'][:2]
```

We need to change the dictionary from **value:list** to **value:string**. We can do that with a function.

```python
def combine_text(list_of_text):
    combined_text = ' '.join(list_of_text)
    return combined_text
  
data_combined = {key: [combine_text(value)] for (key, value) in data.items()}
```

Next we can put everything into a Pandas DataFrame. 

```python
import pandas as pd

pd.set_option('max_colwidth', 150)
data_df = pd.DataFrame.from_dict(data_combined).transpose()
data_df.columns = ['transcript']
data_df
```

Now you get a small glimpse of what the transcript look like, we now need to clean up all the text. By that, it means we need to get rid all the things that are irrelevant to our analysis such as punctuations, capitalization, numerical values, symbols...etc. The most effective method to do this is by using **regular expressions**. We have already done a little bit **regex** in the previous exercise so we will not go too in-depth about the overall concept, we will just concentrate on the specific syntax used here and explain what it achieves. 

```python
import re, string

def clean_text_round1(text):
    text = text.lower()
    text = re.sub('\[.*?\]', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\w*\d\w*', '', text)
    text = re.sub('[^A-Za-z0-9 ]+', '', text)
    return text

round1 = lambda x: clean_text_round1(x)
```

The **regex** routine is written as a function call **clean_text_round1**. The first line removes capitalization. The second removes anything in square brackets, `\` is an escape character so this -`\[   \]` means to treat the square brackets is something to search for and not something as part of the programming language. `.` is any character including any alphanumeric character, `*` is to match the expression to the left 0 or more times, `?` is to match the expression to the left 0 or 1 times. In all `\[.*?\]` means match anything that's within square brackets and replace it with `''` which means empty string. The third line removes punctuations. The fourth line removes any words containing numbers. `\d` is any digits sandwiched between this - `\w` - any alphanumeric characters `*` repeated 0 or more times. 

Once cleaning is done, its time to pickle.

```python
data_df.to_pickle('corpus.pkl')
```

Now is the important step which is to **vectorize** the text. 

```python
from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer(stop_words = 'english')
data_cv = cv.fit_transform(data_clean.transcript)
data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())
data_dtm.index = data_clean.index
data_dtm
```

Last but not least, pickle all the data generated thus far.

```python
data_dtm.to_pickle('dtm.pkl')
data_clean.to_pickle('data_clean.pkl')
pickle.dump(cv, open('cv.pkl', 'wb'))
```

***

# Step 2
## Exploratory Data Analysis

QGIS is an opensource GIS platform that has a very active community of developers constantly making updates and changes. It is a very powerful tool that rivals any commercial software. Download and install the current version, as of writing, the stable release is 3.4. As a suggestion, only install stable releases because you may encounter compatibility issues sometimes. We will not go through specific installation instructions, there should be plenty of online materials if you need assistance.

When you are done installing the software, **launch QGIS**. On the upper left corner, click **Project > New**, you should get a blank screen as the following.



***

# Summary

### What You have Learned

* How to create and assign value to a variable
* How to create and assign values a list
* How to create and assign values a list of lists
* How to bring data into Python as text or csv files 
* How to create and use a counter
* How to write a basic function
* How to call a basic function
* Basic loop structure - how to use for-loops
* How to import packages in Python
* How to use basic functions of packages like Pandas, Plotly, BeautifulSoup
* How to create interactive plots with Plotly.